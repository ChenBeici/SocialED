import argparse
import os
import pandas as pd
import numpy as np
from gensim.models.ldamodel import LdaModel
from gensim import corpora
from sklearn.model_selection import train_test_split
from sklearn import metrics
import logging

# Setup logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

class args_define():
    parser = argparse.ArgumentParser()
    # Hyper-parameters for LDA
    parser.add_argument('--num_topics', default=10, type=int,
                        help="Number of topics to be generated by LDA.")
    parser.add_argument('--passes', default=20, type=int,
                        help="Number of passes through the entire corpus during training.")
    parser.add_argument('--iterations', default=50, type=int,
                        help="Number of iterations for the EM optimization.")
    parser.add_argument('--alpha', default='symmetric', type=str,
                        help="Dirichlet prior for the document-topic distribution.")
    parser.add_argument('--eta', default=None, type=str,
                        help="Dirichlet prior for the topic-word distribution.")
    parser.add_argument('--random_state', default=1, type=int,
                        help="Seed for random number generation to ensure reproducibility.")
    parser.add_argument('--eval_every', default=10, type=int,
                        help="Frequency of evaluation during training.")
    parser.add_argument('--chunksize', default=2000, type=int,
                        help="Number of documents to be processed in each training chunk.")
    parser.add_argument('--model_path', default='./LDA/lda_model.model', type=str,
                        help="Path to save/load the LDA model.")

    # Other arguments
    parser.add_argument('--use_cuda', dest='use_cuda', default=False,
                        action='store_true',
                        help="Use cuda")
    parser.add_argument('--data_path', default='./incremental_test_100messagesperday/', #default='./incremental_0808/',
                        type=str, help="Path of features, labels and edges")
    parser.add_argument('--mask_path', default=None,
                        type=str, help="File path that contains the training, validation and test masks")
    parser.add_argument('--resume_path', default=None,
                        type=str,
                        help="File path that contains the partially performed experiment that needs to be resume.")
    parser.add_argument('--resume_point', default=0, type=int,
                        help="The block model to be loaded.")
    parser.add_argument('--resume_current', dest='resume_current', default=True,
                        action='store_false',
                        help="If true, continue to train the resumed model of the current block(to resume a partally trained initial/mantenance block);\
                            If false, start the next(infer/predict) block from scratch;")
    parser.add_argument('--log_interval', default=10, type=int,
                        help="Log interval")

    args = parser.parse_args()

class LDA:
    def __init__(self, args, dataset):
        self.args = args
        self.dataset = dataset
        self.num_topics = self.args.num_topics
        self.passes = self.args.passes
        self.iterations = self.args.iterations
        self.alpha = self.args.alpha
        self.eta = self.args.eta
        self.random_state = self.args.random_state
        self.eval_every = self.args.eval_every
        self.chunksize = self.args.chunksize
        self.model_path = self.args.model_path
        self.df = None
        self.train_df = None
        self.test_df = None

    def preprocess(self):
        """
        Data preprocessing: tokenization, stop words removal, etc.
        """
        df = pd.DataFrame(self.dataset, columns=[
            "event_id", "tweet_id", "text", "user_id", "created_at", "user_loc",
            "place_type", "place_full_name", "place_country_code", "hashtags", 
            "user_mentions", "image_urls", "entities", "words", "filtered_words", 
            "sampled_words"
        ])
        df['processed_text'] = df['filtered_words'].apply(lambda x: [str(word).lower() for word in x] if isinstance(x, list) else [])
        self.df = df
        return df

    def create_corpus(self, df, text_column):
        """
        Create corpus and dictionary required for LDA model.
        """
        texts = df[text_column].tolist()
        dictionary = corpora.Dictionary(texts)
        corpus = [dictionary.doc2bow(text) for text in texts]
        return corpus, dictionary

    def fit(self):
        """
        Train the LDA model and save it to a file.
        """
        # Ensure the directory exists
        os.makedirs(os.path.dirname(self.model_path), exist_ok=True)

        train_df, test_df = train_test_split(self.df, test_size=0.2, random_state=self.random_state)
        self.train_df = train_df
        self.test_df = test_df
        train_corpus, train_dictionary = self.create_corpus(train_df, 'processed_text')

        logging.info("Training LDA model...")
        lda_model = LdaModel(corpus=train_corpus, id2word=train_dictionary, num_topics=self.num_topics, passes=self.passes,
                             iterations=self.iterations, alpha=self.alpha, eta=self.eta, random_state=self.random_state,
                             eval_every=self.eval_every, chunksize=self.chunksize)
        logging.info("LDA model trained successfully.")
        
        # Save the trained model to a file
        lda_model.save(self.model_path)
        logging.info(f"LDA model saved to {self.model_path}")
        
        self.lda_model = lda_model
        self.train_dictionary = train_dictionary
        return lda_model

    def load_model(self):
        """
        Load the LDA model from a file.
        """
        logging.info(f"Loading LDA model from {self.model_path}...")
        lda_model = LdaModel.load(self.model_path)
        logging.info("LDA model loaded successfully.")
        
        self.lda_model = lda_model
        return lda_model

    def display_topics(self, num_words=10):
        """
        Display topics generated by the LDA model.
        """
        topics = self.lda_model.show_topics(num_words=num_words, formatted=False)
        for i, topic in topics:
            print(f"Topic {i}: {[word for word, _ in topic]}")

    def prediction(self):
        """
        Assign topics to each document.
        """
        self.load_model()  # Ensure the model is loaded before making predictions
        corpus, _ = self.create_corpus(self.test_df, 'processed_text')
        topics = [self.lda_model.get_document_topics(bow) for bow in corpus]
        return topics

    def evaluate_model(self):
        """
        Evaluate the LDA model.
        """
        corpus, _ = self.create_corpus(self.test_df, 'processed_text')
        topics = self.prediction()
        ground_truths = self.test_df['event_id'].tolist()
        predicted_labels = [max(topic, key=lambda x: x[1])[0] for topic in topics]

        # Calculate Adjusted Rand Index (ARI)
        ari = metrics.adjusted_rand_score(ground_truths, predicted_labels)
        print(f"Adjusted Rand Index (ARI): {ari}")

        # Calculate Adjusted Mutual Information (AMI)
        ami = metrics.adjusted_mutual_info_score(ground_truths, predicted_labels)
        print(f"Adjusted Mutual Information (AMI): {ami}")

        # Calculate Normalized Mutual Information (NMI)
        nmi = metrics.normalized_mutual_info_score(ground_truths, predicted_labels)
        print(f"Normalized Mutual Information (NMI): {nmi}")

        return ari, ami, nmi

# Main function
if __name__ == "__main__":
    from Event2012 import Event2012_Dataset

    dataset = Event2012_Dataset.load_data()
    args = args_define.args

    lda = LDA(args, dataset)
    
    # Data preprocessing
    lda.preprocess()
    
    # Train the model
    lda.fit()
    
    # Prediction
    predictions = lda.prediction()
    #print(predictions)
